{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jscofGEtP8xx"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "!pip3 install --upgrade pip\n",
    "!pip install -U numpy==1.24\n",
    "!pip install -U t5==0.9.2\n",
    "!pip install -U flax\n",
    "!pip install -U jax jaxlib\n",
    "# Restart the runtime after install.\n",
    "# Upload adc.json and operative_config.json to folder.\n",
    "\n",
    "# These fix a dependency issue for tensorflow_gcs_config:\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install tensorflow==2.12.0\n",
    "!pip install -U tensorflow-text==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySw5x--hs-a7"
   },
   "outputs": [],
   "source": [
    "# This import was moved here from below to ensure the dependency is fixed before going forward:\n",
    "import tensorflow_gcs_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2h1MRzBLtex2",
    "outputId": "280bb3ca-8cdd-41f3-f0a8-4e9c21d5be6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU: grpc://10.79.50.146:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import t5\n",
    "\n",
    "#Set the base dir(Google cloud bucket)\n",
    "#Made sure to use a valid GCS Bucket containing the datasets\n",
    "BASE_DIR = \"gs://tse_extension\"  #@param { type: \"string\" }\n",
    "\n",
    "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
    "    raise ValueError(\"You must enter a BASE_DIR.\")\n",
    "ON_CLOUD = True\n",
    "\n",
    "if ON_CLOUD:\n",
    "    from google.colab import auth\n",
    "    # Set credentials for GCS reading/writing from Colab and TPU.\n",
    "    TPU_TOPOLOGY = \"2x2\"\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "        TPU_ADDRESS = tpu.get_master()\n",
    "        print('Running on TPU:', TPU_ADDRESS)\n",
    "    except ValueError:\n",
    "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "    auth.authenticate_user()\n",
    "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
    "    tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Improve logging.\n",
    "from contextlib import contextmanager\n",
    "import logging as py_logging\n",
    "\n",
    "if ON_CLOUD:\n",
    "    tf.get_logger().propagate = False\n",
    "    py_logging.root.setLevel('INFO')\n",
    "\n",
    "@contextmanager\n",
    "def tf_verbosity_level(level):\n",
    "    og_level = tf.logging.get_verbosity()\n",
    "    tf.logging.set_verbosity(level)\n",
    "    yield\n",
    "    tf.logging.set_verbosity(og_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glLJUm1dxIiH"
   },
   "outputs": [],
   "source": [
    "# Multiple unique datafiles:\n",
    "# BF (Small)\n",
    "# tsv_path_bf_small_0 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_0.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_1 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_1.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_2 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_2.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_3 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_3.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_4 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_4.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_5 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_5.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_6 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_6.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_7 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_7.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_8 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_8.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_small_9 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_s_categorized/categorical_data/bf_s_training_9.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "# }\n",
    "# examples_bf_small = dict(train=42558, validation=5835)\n",
    "# tsv_path_bf_small = [\n",
    "#     tsv_path_bf_small_0,\n",
    "#     tsv_path_bf_small_1,\n",
    "#     tsv_path_bf_small_2,\n",
    "#     tsv_path_bf_small_3,\n",
    "#     tsv_path_bf_small_4,\n",
    "#     tsv_path_bf_small_5,\n",
    "#     tsv_path_bf_small_6,\n",
    "#     tsv_path_bf_small_7,\n",
    "#     tsv_path_bf_small_8,\n",
    "#     tsv_path_bf_small_9,\n",
    "# ]\n",
    "\n",
    "# #BF (Medium):\n",
    "# # Multiple unique datafiles:\n",
    "# tsv_path_bf_medium_0 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_0.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_1 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_1.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_2 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_2.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_3 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_3.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_4 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_4.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_5 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_5.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_6 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_6.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_7 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_7.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_8 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_8.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# tsv_path_bf_medium_9 = {\n",
    "#     \"train\":      'gs://amh_t5_test/bf_m_categorized/categorical_data/bf_m_training_9.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "# }\n",
    "# examples_bf_medium = dict(train=52276, validation=6546)\n",
    "# tsv_path_bf_medium = [\n",
    "#     tsv_path_bf_medium_0,\n",
    "#     tsv_path_bf_medium_1,\n",
    "#     tsv_path_bf_medium_2,\n",
    "#     tsv_path_bf_medium_3,\n",
    "#     tsv_path_bf_medium_4,\n",
    "#     tsv_path_bf_medium_5,\n",
    "#     tsv_path_bf_medium_6,\n",
    "#     tsv_path_bf_medium_7,\n",
    "#     tsv_path_bf_medium_8,\n",
    "#     tsv_path_bf_medium_9,\n",
    "# ]\n",
    "\n",
    "# #Mutant Generation:\n",
    "# # Multiple unique datafiles:\n",
    "# tsv_path_mg_0 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_0.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_1 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_1.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_2 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_2.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_3 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_3.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_4 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_4.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_5 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_5.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_6 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_6.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_7 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_7.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_8 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_8.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# tsv_path_mg_9 = {\n",
    "#     \"train\":      'gs://amh_t5_test/mg_categorized/categorical_data/mg_training_9.tsv',\n",
    "#     \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "# }\n",
    "# examples_mg = dict(train=92411, validation=6546)\n",
    "# tsv_path_mg = [\n",
    "#     tsv_path_mg_0,\n",
    "#     tsv_path_mg_1,\n",
    "#     tsv_path_mg_2,\n",
    "#     tsv_path_mg_3,\n",
    "#     tsv_path_mg_4,\n",
    "#     tsv_path_mg_5,\n",
    "#     tsv_path_mg_6,\n",
    "#     tsv_path_mg_7,\n",
    "#     tsv_path_mg_8,\n",
    "#     tsv_path_mg_9,\n",
    "# ]\n",
    "#Single data file:\n",
    "tsv_path_bf_small = {\n",
    "    \"train\":      'gs://amh_t5_test/bfs_train_stacked.tsv',\n",
    "    \"validation\": 'gs://amh_t5_test/bf_s_eval.tsv'\n",
    "}\n",
    "examples_bf_small = dict(train=425580, validation=5835)\n",
    "tsv_path_bf_medium = {\n",
    "    \"train\":      'gs://amh_t5_test/bfm_train_stacked.tsv',\n",
    "    \"validation\": 'gs://amh_t5_test/bf_m_eval.tsv'\n",
    "}\n",
    "examples_bf_medium = dict(train=110239, validation=5835)\n",
    "tsv_path_mg = {\n",
    "    \"train\":      'gs://amh_t5_test/mg_train_sortedsingle.tsv',\n",
    "    \"validation\": 'gs://amh_t5_test/mg_eval.tsv'\n",
    "}\n",
    "examples_mg = dict(train=112119, validation=5835)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PobLvzL18zzR"
   },
   "outputs": [],
   "source": [
    "from t5.data import postprocessors as t5_postprocessors\n",
    "from t5.seqio import Feature,SentencePieceVocabulary\n",
    "\n",
    "\n",
    "# # Set the path of sentencepiece model and vocab files\n",
    "# # Must be the same used for the pre-trained phase\n",
    "vocab_model_path = 'gs://amh_t5_test/dl4se_vocab.model' #@param { type: \"string\" }\n",
    "\n",
    "TaskRegistry = t5.data.TaskRegistry\n",
    "TfdsTask = t5.data.TfdsTask\n",
    "\n",
    "\n",
    "def get_default_vocabulary():\n",
    "    return SentencePieceVocabulary(vocab_model_path, 100)\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": Feature(\n",
    "        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n",
    "\n",
    "    \"targets\": Feature(\n",
    "        vocabulary=get_default_vocabulary(), add_eos=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svsHGh0HwWH_",
    "outputId": "3fdee377-b34f-4628-b9ec-ef50b363a08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few raw valid examples...\n",
      "{'X': b'\"public java.util.List < TYPE_1 > METHOD_1 ( ) { java.util.ArrayList < TYPE_1 > VAR_1 = new java.util.ArrayList < TYPE_1 > ( ) ; for ( TYPE_2 VAR_2 : VAR_3 ) { VAR_1 . METHOD_2 ( VAR_2 . METHOD_1 ( ) ) ; } return VAR_1 ; }\"', 'Y': b'\"public java.util.List < TYPE_1 > METHOD_1 ( ) { return VAR_1 ; }\"'}\n",
      "{'X': b'\"public TYPE_1 < TYPE_2 > METHOD_1 ( TYPE_3 VAR_1 , java.lang.String VAR_2 ) { return METHOD_1 ( VAR_1 . toString ( ) , VAR_2 ) ; }\"', 'Y': b'\"public TYPE_1 < TYPE_2 > METHOD_1 ( TYPE_3 VAR_1 , java.lang.String VAR_2 , java.util.HashMap < java.lang.String , java.lang.String > parameters ) { return METHOD_1 ( VAR_1 . toString ( ) , VAR_2 , parameters ) ; }\"'}\n",
      "{'X': b'\"public static void main ( java.lang.String [ ] args ) throws java.lang.Exception { TYPE_1 VAR_1 = new TYPE_1 ( ) ; VAR_1 . METHOD_1 ( ) ; VAR_1 . add ( VAR_2 ) ; VAR_1 . METHOD_2 ( true ) ; VAR_1 . init ( STRING_1 ) ; }\"', 'Y': b'\"public static void main ( java.lang.String [ ] args ) throws java.lang.Exception { TYPE_1 VAR_1 = new TYPE_1 ( ) ; VAR_1 . METHOD_1 ( ) ; VAR_1 . METHOD_2 ( true ) ; VAR_1 . init ( STRING_1 ) ; }\"'}\n",
      "{'X': b'\"public void METHOD_1 ( int VAR_1 , java.lang.String VAR_2 , long VAR_3 ) { if ( VAR_1 == 0 ) { VAR_4 . METHOD_2 ( java.lang.String . METHOD_3 ( VAR_3 ) ) ; } else { VAR_4 . error ( VAR_2 ) ; } }\"', 'Y': b'\"public void METHOD_1 ( int VAR_1 , java.lang.String VAR_2 , long VAR_3 ) { if ( VAR_1 == 0 ) { VAR_4 . METHOD_2 ( java.lang.String . METHOD_3 ( VAR_3 ) ) ; } else { VAR_4 . error ( VAR_1 ) ; } }\"'}\n",
      "{'X': b'\"public void METHOD_1 ( java.lang.Integer VAR_1 ) { VAR_2 . METHOD_2 ( ) . METHOD_3 ( ) ; this . VAR_3 . remove ( VAR_1 ) ; VAR_2 . METHOD_2 ( ) . METHOD_4 ( ) ; }\"', 'Y': b'\"public void METHOD_1 ( java.lang.Integer VAR_1 ) { this . VAR_3 . remove ( VAR_1 ) ; }\"'}\n",
      "A few raw valid examples...\n",
      "{'X': b'\"private void METHOD_1 ( TYPE_1 VAR_1 ) { VAR_2 . METHOD_2 ( STRING_1 ) ; TYPE_2 out = VAR_1 . METHOD_3 ( ) . buffer ( ) ; TYPE_3 VAR_3 = new TYPE_3 ( ) ; VAR_4 = INT_1 ; VAR_3 . METHOD_4 ( VAR_5 ) ; out . METHOD_5 ( VAR_3 . METHOD_6 ( ) ) ; VAR_1 . METHOD_7 ( out ) ; }\"', 'Y': b'\"private void METHOD_1 ( TYPE_1 VAR_1 ) { VAR_2 . info ( STRING_1 ) ; TYPE_2 out = VAR_1 . METHOD_3 ( ) . buffer ( ) ; TYPE_3 VAR_3 = new TYPE_3 ( ) ; VAR_4 = INT_1 ; VAR_3 . METHOD_4 ( VAR_5 ) ; out . METHOD_5 ( VAR_3 . METHOD_6 ( ) ) ; VAR_1 . METHOD_7 ( out ) ; }\"'}\n",
      "{'X': b'\"public void METHOD_1 ( ) { TYPE_1 VAR_1 = new TYPE_1 ( INT_1 ) ; VAR_1 . METHOD_2 ( STRING_1 ) ; TYPE_2 VAR_2 = new TYPE_2 ( ) ; TYPE_3 . METHOD_3 ( STRING_2 , ( ( VAR_2 . METHOD_4 ( new TYPE_4 ( INT_2 , INT_3 ) , VAR_3 , VAR_1 ) ) == true ) ) ; }\"', 'Y': b'\"public void METHOD_1 ( ) { TYPE_1 VAR_1 = new TYPE_1 ( INT_1 ) ; VAR_1 . METHOD_2 ( STRING_1 ) ; TYPE_2 VAR_2 = new TYPE_2 ( ) ; TYPE_3 . assertTrue ( STRING_2 , ( ( VAR_2 . METHOD_4 ( new TYPE_4 ( INT_2 , INT_3 ) , VAR_3 , VAR_1 ) ) == false ) ) ; }\"'}\n",
      "{'X': b'\"public void METHOD_1 ( java.lang.String title , java.lang.String msg ) { VAR_1 . info ( ( STRING_1 + title ) ) ; try { TYPE_1 . METHOD_2 ( VAR_2 ) . METHOD_3 ( TYPE_2 . METHOD_4 ( ) . add ( STRING_2 , msg ) . build ( ) , VAR_3 ) . execute ( ) ; } catch ( java.io.IOException VAR_4 ) { VAR_1 . error ( STRING_3 , VAR_4 ) ; } }\"', 'Y': b'\"public void METHOD_1 ( java.lang.String title , java.lang.String msg ) { VAR_1 . info ( ( STRING_1 + title ) ) ; try { TYPE_1 . METHOD_2 ( VAR_2 ) . METHOD_3 ( TYPE_2 . METHOD_4 ( ) . add ( STRING_2 , title ) . build ( ) , VAR_3 ) . execute ( ) ; } catch ( java.io.IOException VAR_4 ) { VAR_1 . error ( STRING_3 , VAR_4 ) ; } }\"'}\n",
      "{'X': b'\"public void METHOD_1 ( TYPE_1 VAR_1 ) { if ( ( ( ( ( ( ! ( VAR_2 ) ) && ( ( METHOD_2 ( ) ) != null ) ) && ( ( METHOD_3 ( ) ) != null ) ) && ( ( VAR_3 ) != null ) ) && ( METHOD_4 ( VAR_4 ) ) ) && ( METHOD_5 ( ) ) ) { VAR_2 = true ; } else if ( VAR_5 ) METHOD_6 ( VAR_1 ) ; }\"', 'Y': b'\"public void METHOD_1 ( TYPE_1 VAR_1 ) { if ( ( ( ( ( ( ! ( VAR_2 ) ) && ( ( METHOD_2 ( ) ) != null ) ) && ( ( METHOD_3 ( ) ) != null ) ) && ( ( VAR_3 ) != null ) ) && ( METHOD_4 ( VAR_4 ) ) ) && ( METHOD_5 ( ) ) ) { VAR_2 = true ; } else METHOD_6 ( VAR_1 ) ; }\"'}\n",
      "{'X': b'\"public TYPE_1 METHOD_1 ( ) { try { if ( VAR_1 . METHOD_2 ( INT_1 ) ) { return VAR_1 ; } else { VAR_1 = TYPE_2 . METHOD_3 ( METHOD_4 ( ) , VAR_2 , VAR_3 ) ; return VAR_1 ; } } catch ( TYPE_3 VAR_4 ) { VAR_4 . METHOD_5 ( ) ; return null ; } }\"', 'Y': b'\"public TYPE_1 METHOD_1 ( ) { try { if ( ( ( VAR_1 ) != null ) && ( VAR_1 . METHOD_2 ( INT_1 ) ) ) { return VAR_1 ; } else { VAR_1 = TYPE_2 . METHOD_3 ( METHOD_4 ( ) , VAR_2 , VAR_3 ) ; return VAR_1 ; } } catch ( TYPE_3 VAR_4 ) { VAR_4 . METHOD_5 ( ) ; return null ; } }\"'}\n",
      "A few raw valid examples...\n",
      "{'X': b'\"public void METHOD_1 ( java.lang.String VAR_1 ) { try { java.lang.String VAR_2 = ( STRING_1 + VAR_1 ) + STRING_2 ; METHOD_2 ( VAR_2 ) ; } catch ( TYPE_1 VAR_3 ) { VAR_3 . METHOD_3 ( ) ; } }\"', 'Y': b'\"public void METHOD_1 ( java.lang.String VAR_1 ) { try { java.lang.String VAR_2 = ( STRING_3 + VAR_1 ) + STRING_2 ; METHOD_2 ( VAR_2 ) ; } catch ( TYPE_1 VAR_3 ) { VAR_3 . METHOD_3 ( ) ; } }\"'}\n",
      "{'X': b'\"public boolean execute ( ) { METHOD_1 ( ) ; if ( VAR_1 . isEmpty ( ) ) { return true ; } return false ; }\"', 'Y': b'\"public boolean execute ( ) { METHOD_1 ( ) ; if ( ( VAR_1 . size ( ) ) == 0 ) { return true ; } return false ; }\"'}\n",
      "{'X': b'\"public java.lang.String toString ( ) { return ( ( ( ( STRING_1 + ( VAR_1 ) ) + STRING_2 ) + ( title ) ) + STRING_3 ) + ( VAR_2 ) ; }\"', 'Y': b'\"public java.lang.String toString ( ) { return ( ( STRING_4 + ( title ) ) + STRING_3 ) + ( VAR_2 ) ; }\"'}\n",
      "{'X': b'\"public void METHOD_1 ( final java.lang.String VAR_1 ) { final java.util.Map < java.lang.String , TYPE_1 > VAR_2 = new java.util.HashMap ( this . VAR_2 ) ; VAR_3 . remove ( VAR_1 ) ; this . VAR_2 = VAR_2 ; }\"', 'Y': b'\"public void METHOD_1 ( final java.lang.String VAR_1 ) { VAR_3 . remove ( VAR_1 ) ; }\"'}\n",
      "{'X': b'\"public void METHOD_1 ( int VAR_1 ) { if ( ( ( state ) == ( VAR_2 ) ) || ( ( state ) == ( VAR_3 ) ) ) this . VAR_4 = VAR_1 ; else this . VAR_1 = VAR_1 ; }\"', 'Y': b'\"public void METHOD_1 ( int VAR_1 ) { if ( ( state ) == ( VAR_2 ) ) this . VAR_4 = VAR_1 ; else this . VAR_1 = VAR_1 ; }\"'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<t5.data.dataset_providers.FunctionTask at 0x7eaab5517a60>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nq_dataset_bfp_small_single(split, shuffle_files=False):\n",
    "    del shuffle_files\n",
    "\n",
    "    # Load lines from the text file as examples.\n",
    "    ds = tf.data.TextLineDataset(tsv_path_bf_small[split])\n",
    "    ds = ds.map(\n",
    "        functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
    "                        field_delim=\"\\t\", use_quote_delim=False),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds = ds.map(lambda *ex: dict(zip([\"X\", \"Y\"], ex)))\n",
    "    return ds\n",
    "\n",
    "\n",
    "print(\"A few raw valid examples...\")\n",
    "for ex in tfds.as_numpy(nq_dataset_bfp_small_single(\"validation\").take(5)):\n",
    "    print(ex)\n",
    "\n",
    "def bfp_preprocessing_small_single(ds):\n",
    "\n",
    "    def to_inputs_and_targets(ex):\n",
    "\n",
    "        inputs = tf.strings.join(['generate small patch: '  + ex['X']], separator=' ')\n",
    "        class_label = tf.strings.join([ex['Y']], separator=' ')\n",
    "        return {'inputs': inputs, 'targets': class_label }\n",
    "\n",
    "    return ds.map(to_inputs_and_targets,\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "TaskRegistry = t5.data.TaskRegistry\n",
    "TfdsTask = t5.data.TfdsTask\n",
    "\n",
    "t5.data.TaskRegistry.remove('bfp_small_single')\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"bfp_small_single\",\n",
    "    dataset_fn=nq_dataset_bfp_small_single,\n",
    "    splits=[\"train\", \"validation\"],\n",
    "    text_preprocessor=[bfp_preprocessing_small_single],\n",
    "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
    "    num_input_examples = examples_bf_small\n",
    ")\n",
    "\n",
    "def nq_dataset_bfp_medium_single(split, shuffle_files=False):\n",
    "    del shuffle_files\n",
    "\n",
    "    # Load lines from the text file as examples.\n",
    "    ds = tf.data.TextLineDataset(tsv_path_bf_medium[split])\n",
    "    ds = ds.map(\n",
    "        functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
    "                        field_delim=\"\\t\", use_quote_delim=False),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds = ds.map(lambda *ex: dict(zip([\"X\", \"Y\"], ex)))\n",
    "    return ds\n",
    "\n",
    "\n",
    "print(\"A few raw valid examples...\")\n",
    "for ex in tfds.as_numpy(nq_dataset_bfp_medium_single(\"validation\").take(5)):\n",
    "    print(ex)\n",
    "\n",
    "def bfp_preprocessing_medium_single(ds):\n",
    "\n",
    "    def to_inputs_and_targets(ex):\n",
    "\n",
    "        inputs = tf.strings.join(['generate medium patch: '  + ex['X']], separator=' ')\n",
    "        class_label = tf.strings.join([ex['Y']], separator=' ')\n",
    "        return {'inputs': inputs, 'targets': class_label }\n",
    "\n",
    "    return ds.map(to_inputs_and_targets,\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "TaskRegistry = t5.data.TaskRegistry\n",
    "TfdsTask = t5.data.TfdsTask\n",
    "\n",
    "t5.data.TaskRegistry.remove('bfp_medium_single')\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"bfp_medium_single\",\n",
    "    dataset_fn=nq_dataset_bfp_medium_single,\n",
    "    splits=[\"train\", \"validation\"],\n",
    "    text_preprocessor=[bfp_preprocessing_medium_single],\n",
    "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
    "    num_input_examples = examples_bf_medium\n",
    ")\n",
    "\n",
    "def nq_dataset_mg_single(split, shuffle_files=False):\n",
    "    del shuffle_files\n",
    "\n",
    "    # Load lines from the text file as examples.\n",
    "    ds = tf.data.TextLineDataset(tsv_path_mg[split])\n",
    "    ds = ds.map(\n",
    "        functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
    "                        field_delim=\"\\t\", use_quote_delim=False),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    ds = ds.map(lambda *ex: dict(zip([\"X\", \"Y\"], ex)))\n",
    "    return ds\n",
    "\n",
    "\n",
    "print(\"A few raw valid examples...\")\n",
    "for ex in tfds.as_numpy(nq_dataset_mg_single(\"validation\").take(5)):\n",
    "    print(ex)\n",
    "\n",
    "def mg_preprocessing_single(ds):\n",
    "\n",
    "    def to_inputs_and_targets(ex):\n",
    "\n",
    "        inputs = tf.strings.join(['generate mutant: '  + ex['X']], separator=' ')\n",
    "        class_label = tf.strings.join([ex['Y']], separator=' ')\n",
    "        return {'inputs': inputs, 'targets': class_label }\n",
    "\n",
    "    return ds.map(to_inputs_and_targets,\n",
    "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "TaskRegistry = t5.data.TaskRegistry\n",
    "TfdsTask = t5.data.TfdsTask\n",
    "\n",
    "t5.data.TaskRegistry.remove('mg_single')\n",
    "t5.data.TaskRegistry.add(\n",
    "    \"mg_single\",\n",
    "    dataset_fn=nq_dataset_mg_single,\n",
    "    splits=[\"train\", \"validation\"],\n",
    "    text_preprocessor=[mg_preprocessing_single],\n",
    "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
    "    num_input_examples = examples_mg\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3Qx699vN302"
   },
   "outputs": [],
   "source": [
    "# MODEL_DIR is where to store the new model.\n",
    "# PRETRAINED_MODEL is where the existing model is.\n",
    "# The pretrained model dir must have the operative_config.gin.\n",
    "\n",
    "import t5.models\n",
    "from mesh_tensorflow.transformer.learning_rate_schedules import truncated_rsqrt\n",
    "\n",
    "MODEL_SIZE = \"small\"\n",
    "MODEL_DIR = 'gs://amh_t5_test/CURRENT/pt_cl_mg_sched14g/'\n",
    "#MODEL_DIR = 'gs://amh_t5_test/CURRENT/pretrained_bfs_25ep_temp1/' #@param { type: \"string\" }\n",
    "PRETRAINED_MODEL = 'gs://amh_t5_test/CURRENT/pt_cl_mg_sched14f'\n",
    "#PRETRAINED_MODEL = 'gs://amh_t5_test/pretrain_250k/'\n",
    "#PRETRAINED_MODEL = 'gs://amh_t5_test/baseline/pretrain/' #@param { type: \"string\" }\n",
    "NUM_EPOCH = 100\n",
    "DATASET_SIZE = 112119\n",
    "\n",
    "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
    "    \"small\": (1, 256, 200),\n",
    "    \"base\": (2, 128, 8),\n",
    "    \"large\": (8, 64, 4),\n",
    "    \"3B\": (8, 16, 1),\n",
    "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
    "\n",
    "tf.io.gfile.makedirs(MODEL_DIR)\n",
    "\n",
    "model = t5.models.MtfModel(\n",
    "    model_dir=MODEL_DIR,\n",
    "    tpu=TPU_ADDRESS,\n",
    "    #tpu_topology=TPU_TOPOLOGY,\n",
    "    model_parallelism=model_parallelism,\n",
    "    batch_size=train_batch_size,\n",
    "    learning_rate_schedule = truncated_rsqrt,\n",
    "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
    "    save_checkpoints_steps=10000,\n",
    "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
    "    iterations_per_loop=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRp9EEMDPPQG",
    "outputId": "f422001e-8c7e-463b-8f25-b7259d7ef787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGS: ['/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py']\n"
     ]
    }
   ],
   "source": [
    "# Note: This block is only necessary if you get a \"f flag\" error. (encountered on Colab)\n",
    "# It is safe to run on Colab if not encountering the error.\n",
    "import sys\n",
    "\n",
    "print(\"ARGS:\", sys.argv)\n",
    "old_args = sys.argv\n",
    "sys.argv = [old_args[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVHK3H5tUQwR"
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "import numpy as np\n",
    "import mesh_tensorflow as mtf\n",
    "from mesh_tensorflow import placement_mesh_impl\n",
    "import math\n",
    "\n",
    "def fibonacci_rounded(number):\n",
    "    rounded_number = int(number)  # Round down the number\n",
    "    fib_sequence = [0, 1]  # Initialize Fibonacci sequence\n",
    "\n",
    "    # Generate Fibonacci sequence up to the rounded number\n",
    "    while len(fib_sequence) <= rounded_number:\n",
    "        next_fib = fib_sequence[-1] + fib_sequence[-2]\n",
    "        fib_sequence.append(next_fib)\n",
    "\n",
    "    return fib_sequence[rounded_number]  # Return the Fibonacci number\n",
    "\n",
    "gin.enter_interactive_mode() # This allows us to re-assign the CustomContrastiveLoss during development.\n",
    "@gin.configurable(module='tf.losses')\n",
    "class CustomContrastiveLossFibonacci:\n",
    "    def __init__(self, param=0.5):\n",
    "        # Param is margin for margin-based and temperature for temperature-based implementations.\n",
    "        self.param = param\n",
    "        self.epochs = 10\n",
    "        self.datasize = 112119 # replace with length of training dataset\n",
    "        self.batchsize = 256\n",
    "        self.epochs = 100\n",
    "        self.steps = (self.datasize / self.batchsize) * self.epochs\n",
    "        self.steps = math.floor(self.steps)\n",
    "        self.minval = 7\n",
    "        self.maxval = 9 # gives the fib vals 1, 1, 2, 3, 5, 8, 13\n",
    "        self.rate = ((self.maxval - self.minval) / self.steps)\n",
    "        self.temp = self.maxval\n",
    "        # This print statement exists so that we can confirm the custom\n",
    "        # loss function is being called at runtime.\n",
    "        print(\"Using Contrastive Learning with Schedule=\", self.minval, self.maxval)\n",
    "        print(\"Steps: \", self.steps)\n",
    "        print(\"Rate: \", self.rate)\n",
    "        print(self.temp)\n",
    "    # Temperature-based approach\n",
    "    def __call__(self, transformer, context, logits, targets, output_vocab_dim, temperature):\n",
    "        \"\"\"Temperature-based contrastive loss function.\n",
    "\n",
    "        Args:\n",
    "            logits: A mtf.Tensor of shape [outer_batch, batch, length].\n",
    "            targets: A mtf.Tensor of shape [outer_batch, batch, length].\n",
    "            output_vocab_dim: An integer representing the dimension to reduce the logits.\n",
    "            temperature: A float value representing the temperature parameter.\n",
    "\n",
    "        Returns:\n",
    "            A mtf.Tensor of shape [outer_batch, batch] representing the temperature-based contrastive loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the shapes of logits and targets do not match.\n",
    "        \"\"\"\n",
    "        self.temp += self.rate\n",
    "        #self.temp -= self.rate\n",
    "        fib = fibonacci_rounded(self.temp)\n",
    "        targets = mtf.cast(targets, logits.dtype)\n",
    "        reduced_logits = mtf.reduce_sum(logits, reduced_dim=output_vocab_dim)\n",
    "\n",
    "        pairwise_diff = mtf.sub(targets, reduced_logits)\n",
    "        abs_diff = mtf.abs(pairwise_diff)\n",
    "\n",
    "        scaled_logits = mtf.divide(abs_diff, fib)  # Scale logits by temperature\n",
    "        softmax_scores = mtf.softmax(-scaled_logits, dim=output_vocab_dim)  # Apply softmax operation\n",
    "        log_probs = mtf.log(softmax_scores)\n",
    "\n",
    "        loss = -log_probs\n",
    "        mean_loss = mtf.reduce_mean(loss)\n",
    "\n",
    "        return mean_loss\n",
    "\n",
    "    \n",
    "class CustomContrastiveLossLinear:\n",
    "    def __init__(self, param=0.5):\n",
    "        # Param is margin for margin-based and temperature for temperature-based implementations.\n",
    "        self.param = param\n",
    "        self.epochs = 10\n",
    "        self.datasize = 112119 # replace with length of training dataset\n",
    "        self.batchsize = 256\n",
    "        self.epochs = 100\n",
    "        self.steps = (self.datasize / self.batchsize) * self.epochs\n",
    "        self.steps = math.floor(self.steps)\n",
    "        self.minval = 1\n",
    "        self.maxval = 10\n",
    "        self.rate = ((self.maxval - self.minval) / self.steps)\n",
    "        self.temp = self.minval\n",
    "        # This print statement exists so that we can confirm the custom\n",
    "        # loss function is being called at runtime.\n",
    "        print(\"Using Contrastive Learning with Schedule=\", self.minval, self.maxval)\n",
    "        print(\"Steps: \", self.steps)\n",
    "        print(\"Rate: \", self.rate)\n",
    "        print(self.temp)\n",
    "    # Temperature-based approach\n",
    "    def __call__(self, transformer, context, logits, targets, output_vocab_dim, temperature):\n",
    "        \"\"\"Temperature-based contrastive loss function.\n",
    "\n",
    "        Args:\n",
    "            logits: A mtf.Tensor of shape [outer_batch, batch, length].\n",
    "            targets: A mtf.Tensor of shape [outer_batch, batch, length].\n",
    "            output_vocab_dim: An integer representing the dimension to reduce the logits.\n",
    "            temperature: A float value representing the temperature parameter.\n",
    "\n",
    "        Returns:\n",
    "            A mtf.Tensor of shape [outer_batch, batch] representing the temperature-based contrastive loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the shapes of logits and targets do not match.\n",
    "        \"\"\"\n",
    "        self.temp += self.rate\n",
    "\n",
    "        targets = mtf.cast(targets, logits.dtype)\n",
    "        reduced_logits = mtf.reduce_sum(logits, reduced_dim=output_vocab_dim)\n",
    "\n",
    "        pairwise_diff = mtf.sub(targets, reduced_logits)\n",
    "        abs_diff = mtf.abs(pairwise_diff)\n",
    "\n",
    "        scaled_logits = mtf.divide(abs_diff, self.temp)  # Scale logits by temperature\n",
    "        softmax_scores = mtf.softmax(-scaled_logits, dim=output_vocab_dim)  # Apply softmax operation\n",
    "        log_probs = mtf.log(softmax_scores)\n",
    "\n",
    "        loss = -log_probs\n",
    "        mean_loss = mtf.reduce_mean(loss)\n",
    "\n",
    "        return mean_loss\n",
    "\n",
    "    # # Margin-based approach\n",
    "    # def __call__(self, transformer, context, logits, targets, output_vocab_dim):\n",
    "    #     \"\"\"Contrastive loss function.\n",
    "\n",
    "    #     Args:\n",
    "    #         logits: A mtf.Tensor of shape [outer_batch, batch, length].\n",
    "    #         targets: A mtf.Tensor of shape [outer_batch, batch, length].\n",
    "    #         margin: A float value representing the margin for contrastive loss.\n",
    "\n",
    "    #     Returns:\n",
    "    #         A mtf.Tensor of shape [outer_batch, batch] representing the contrastive loss.\n",
    "\n",
    "    #     Raises:\n",
    "    #         ValueError: If the shapes of logits and targets do not match.\n",
    "    #     \"\"\"\n",
    "    #     targets = mtf.cast(targets, logits.dtype)\n",
    "    #     reduced_logits = mtf.reduce_sum(logits, reduced_dim=output_vocab_dim)\n",
    "\n",
    "    #     pairwise_diff = mtf.sub(targets, reduced_logits)\n",
    "    #     abs_diff = mtf.abs(pairwise_diff)\n",
    "    #     modified_abs_diff = mtf.sub(self.param, abs_diff)\n",
    "    #     loss = mtf.maximum(modified_abs_diff, 0)\n",
    "    #     mean_loss = mtf.reduce_mean(loss)\n",
    "\n",
    "    #     return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueNnwFreptE6",
    "outputId": "89085365-f36f-4dd0-91ec-3d2298ad7039"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:system_path_file_exists:gs://amh_t5_test/CURRENT/pt_cl_mg_sched14f/operative_config.gin\n",
      "ERROR:root:Path not found: gs://amh_t5_test/CURRENT/pt_cl_mg_sched14f/operative_config.gin\n",
      "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2043: TPUConfig.__new__ (from tensorflow_estimator.python.estimator.tpu.tpu_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2059: RunConfig.__init__ (from tensorflow_estimator.python.estimator.tpu.tpu_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_config.py:268: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:2096: TPUEstimator.__init__ (from tensorflow_estimator.python.estimator.tpu.tpu_estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:2811: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Contrastive Learning with Schedule= 7 9\n",
      "Steps:  43796\n",
      "Rate:  4.566627089231893e-05\n",
      "9\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:2371: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/basic_session_run_hooks.py:686: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "WARNING:absl:Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).\n",
      "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
      "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
      "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
      "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:999: CheckpointSaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/mesh_tensorflow/transformer/utils.py:1014: TPUEstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.tpu.tpu_estimator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3328: LoggingTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:3369: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/estimator.py:1414: NanTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:586: SummarySaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:760: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n",
      "From /usr/local/lib/python3.10/dist-packages/tensorflow/python/training/monitored_session.py:1474: SessionRunValues.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.keras instead.\n"
     ]
    }
   ],
   "source": [
    "import gin\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH_GIN_FILE = '/content/operative_config.gin'\n",
    "STEPS_PER_EPOCH = int(DATASET_SIZE/train_batch_size)\n",
    "task_list = [\"mg_single\"]\n",
    "\n",
    "# def set_dynamic_task(selection):\n",
    "#     t5.data.TaskRegistry.remove('bfp_small')\n",
    "#     t5.data.TaskRegistry.add(\n",
    "#         \"bfp_small\",\n",
    "#         dataset_fn=lambda split, shuffle_files: nq_dataset_bfp_small(selection, split, shuffle_files),\n",
    "#         splits=[\"train\", \"validation\"],\n",
    "#         text_preprocessor=[bfp_preprocessing_small],\n",
    "#         output_features = DEFAULT_OUTPUT_FEATURES,\n",
    "#         metric_fns=[t5.evaluation.metrics.accuracy],\n",
    "#         num_input_examples = examples_bf_small\n",
    "#     )\n",
    "\n",
    "# for epoch in range(NUM_EPOCH):\n",
    "#     print(\"EPOCH: \", epoch)\n",
    "#     dataset_selection = epoch % 10\n",
    "#     set_dynamic_task(dataset_selection)\n",
    "\n",
    "#     model.batch_size = 128\n",
    "#     with gin.unlock_config():\n",
    "#         gin.parse_config_file(PATH_GIN_FILE)\n",
    "#         # MtfModel determines how much to train based on (PRETRAINED_STEPS) + (finetune_steps)\n",
    "#         # As we keep the same pretrained model, we have to increase the number of finetune_steps\n",
    "#         # in each epoch to provide the new final target.\n",
    "#         # If this isn't done, after the first epoch you will see epochs pass without any processing.\n",
    "#         # This is due to it starting the training, realizing it is at the number of steps (from the first epoch),\n",
    "#         # and stopping.\n",
    "#         model.finetune('bfp_small',\n",
    "#                       finetune_steps=STEPS_PER_EPOCH * (epoch + 1),\n",
    "#                       pretrained_model_dir=PRETRAINED_MODEL,\n",
    "#         )\n",
    "\n",
    "with gin.unlock_config():\n",
    "  gin.parse_config_file(PATH_GIN_FILE)\n",
    "  # MtfModel determines how much to train based on (PRETRAINED_STEPS) + (finetune_steps)\n",
    "  # As we keep the same pretrained model, we have to increase the number of finetune_steps\n",
    "  # in each epoch to provide the new final target.\n",
    "  # If this isn't done, after the first epoch you will see epochs pass without any processing.\n",
    "  # This is due to it starting the training, realizing it is at the number of steps (from the first epoch),\n",
    "  # and stopping.\n",
    "  model.finetune('mg_single',\n",
    "                finetune_steps=43796, #400 epochs at 52861 rows, 128 batch size\n",
    "                pretrained_model_dir=PRETRAINED_MODEL,\n",
    "  )\n",
    "    # model.batch_size = 64\n",
    "    # for task in task_list:\n",
    "\n",
    "    #     model.eval(\n",
    "    #       mixture_or_task_name=task,\n",
    "    #       checkpoint_steps=-1\n",
    "    #       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oHp5ScE7nf2"
   },
   "outputs": [],
   "source": [
    "# # Original baseline fine tuning.\n",
    "\n",
    "# PATH_GIN_FILE = 'operative_config.gin'\n",
    "# STEP = 37371 # Roughly 90 epochs for bf_s, (51350/128)*90\n",
    "\n",
    "# with gin.unlock_config():\n",
    "#     gin.parse_config_file(PATH_GIN_FILE)\n",
    "#     model.finetune('bfp_small',\n",
    "#                    finetune_steps=STEP,\n",
    "#                    pretrained_model_dir=PRETRAINED_MODEL,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAo_rui4HXcT"
   },
   "outputs": [],
   "source": [
    "# # With eval: WIP\n",
    "\n",
    "# import t5.models\n",
    "# from mesh_tensorflow.transformer.learning_rate_schedules import truncated_rsqrt\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# MODEL_SIZE = \"small\"\n",
    "# MODEL_DIR = 'gs://amh_t5_test/bf_s_categorized/categorical_model/' #@param { type: \"string\" }\n",
    "# PRETRAINED_MODEL = 'gs://amh_t5_test/baseline/bf_nopretrain/' #@param { type: \"string\" }\n",
    "\n",
    "# # MODEL_DIR = 'gs://amh_t5_test/no_pt/bf_s' #@param { type: \"string\" }\n",
    "# # PRETRAINED_MODEL = 'gs://amh_t5_test/no_pt' #@param { type: \"string\" }\n",
    "\n",
    "\n",
    "# model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
    "#     \"small\": (1, 128, 200),\n",
    "#     \"base\": (2, 128, 8),\n",
    "#     \"large\": (8, 64, 4),\n",
    "#     \"3B\": (8, 16, 1),\n",
    "#     \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
    "\n",
    "# tf.io.gfile.makedirs(MODEL_DIR)\n",
    "# def set_dynamic_task(selection):\n",
    "#     t5.data.TaskRegistry.remove('bfp_small')\n",
    "#     t5.data.TaskRegistry.add(\n",
    "#         \"bfp_small\",\n",
    "#         dataset_fn=lambda split, shuffle_files: nq_dataset_bfp_small(selection, split, shuffle_files),\n",
    "#         splits=[\"train\", \"validation\"],\n",
    "#         text_preprocessor=[bfp_preprocessing_small],\n",
    "#         output_features = DEFAULT_OUTPUT_FEATURES,\n",
    "#         metric_fns=[t5.evaluation.metrics.accuracy],\n",
    "#         num_input_examples = examples_bf_small\n",
    "#     )\n",
    "# model = t5.models.MtfModel(\n",
    "#     model_dir=MODEL_DIR,\n",
    "#     tpu=TPU_ADDRESS,\n",
    "#     #tpu_topology=TPU_TOPOLOGY,\n",
    "#     model_parallelism=model_parallelism,\n",
    "#     batch_size=64,\n",
    "#     learning_rate_schedule = truncated_rsqrt,\n",
    "#     sequence_length={\"inputs\": 512, \"targets\": 512},\n",
    "#     save_checkpoints_steps=10000,\n",
    "#     keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
    "#     iterations_per_loop=100,\n",
    "# )\n",
    "# set_dynamic_task(0)\n",
    "# # Use a larger batch size for evaluation, which requires less memory.\n",
    "# # For Mutant Generation we rely on TF's predictions with beam size K=1\n",
    "\n",
    "# PATH_GIN_FILE = 'operative_config.gin'\n",
    "# import gin\n",
    "\n",
    "# with gin.unlock_config():\n",
    "#     gin.external_configurable(tfa.losses.ContrastiveLoss, module='tf.losses')\n",
    "#     gin.parse_config_file(PATH_GIN_FILE)\n",
    "#     task_list = [\"bfp_small\"]\n",
    "#     model.batch_size = 64\n",
    "#     for task in task_list:\n",
    "\n",
    "#         model.eval(\n",
    "#           mixture_or_task_name=task,\n",
    "#           checkpoint_steps=-1\n",
    "#           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bfG1gBbs1x2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
